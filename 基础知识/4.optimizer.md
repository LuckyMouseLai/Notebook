### 2.12.7 各种梯度下降法性能比较
​	下表简单对比随机梯度下降（SGD）、批量梯度下降（BGD）、小批量梯度下降（Mini-batch GD）、和Online GD的区别：

||BGD|SGD|Mini-batch GD|Online GD|
|:-:|:-:|:-:|:-:|:-:|:-:|
|训练集|固定|固定|固定|实时更新|
|单次迭代样本数|整个训练集|单个样本|训练集的子集|根据具体算法定|
|算法复杂度|高|低|一般|低|
|时效性|低|一般|一般|高|
|收敛性|稳定|不稳定|较稳定|不稳定|

- 对学习率敏感，太小训练慢，太大超过极值点
- 陷入局部最优

## 动量法：在随机梯度下降的基础上，加入上一次更新时的梯度，更加稳定

## AdaGrad
- 通过参数来调整合适的学习率，是能独立自动调整模型参数的学习率，对稀疏参数进行大幅更新和对频繁参数进行小幅更新，因此，AdaGrad方法非常适合处理稀疏数据
## RMSProp
- 通过修改AdaGrad得来，其目的是在非凸背景下效果更好。针对梯度平方和累计越来越大的问题，RMSProp指数加权的移动平均代替梯度平方和。RMSProp为了使用移动平均，还引入了一个新的超参数ρ \rhoρ，用来控制移动平均的长度范围
## Adam
Adam本质上是带有动量项的RMSProp，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使参数比较稳定。Adam是一种学习速率自适应的深度神经网络方法
