## 图像分类指标
### 改进思路1：加深宽度(通道数)+深度(网络层数)
### 1. LeNet
    - 5层网络，3层卷积+2层全连接

### 2. AlexNet
    - 8层网络，5层卷积+3层全连接
    - 第一层使用11x11大卷积核
    - sigmoid换成relu
    - 使用dropout

### 3. VGG
    - VGG11，8层卷积(5个卷积块，3个双+2个单)+3层全连接
    - 有可重复使用的vgg block(卷积+池化)

### 改进思路2：减少参数量
### 4.Nin
    - 使用1x1的卷积代替全连接
    - nin块 串联连接（1个卷积+2个1x1的卷积)
    - 相比AlexNet，同样使用了大卷积核，11，5，3。但是去除后面全连接，改用nin块，输出通道=类别数，使用全局平均池化获得输出。可以减少模型参数尺寸。

### 5.GoogLeNet - Inception v1
- 使用Inception块(包含4条线路)，前3条抽取不同空间尺寸大小的信息，第2，3条中1x1卷积核用于减少输入通道数。池化层(保留显著特征)
    - p1：1x1卷积核
    - p2：1x1卷积核 + 3x3卷积核
    - p3： 1x1卷积核 + 5x5卷积核
    - p4：3x3最大池化 + 1x1卷积核
- 模型参数小，计算复杂度低
![Inception v1](./pics/5-1.png)

### 6.ResNet
- 使用残差块,resnet18和resnet34使用basic block(2个3x3)，resnet50和renset101使用bottleneck block(2个1x1和1个3x3)，1x1卷积减少通道数，降低计算量
![Resnet block](./pics/5-3.png)
- 各个模型使用的残差块
![Resnet](./pics/5-2.png)
- 优点：缓解梯度消失和梯度爆炸
- 层数计算: 1 (7x7) + block nums*block conv nums + 1(fc)

### 7.DenseNet
![DenseMet](./pics/5-4.png)
- 建立前面层和后面层的密集连接，在通道上拼接实现特征重用。
- Dense block 和 Transition（过渡层）
    - Dense Basic block：BN-ReLU-3x3Conv
    - Dense Bottleneck block: BN-ReLU-1x1Conv +  BN-ReLU-3x3Conv
    - Transition Layer: 1x1Conv-pool。1x1Conv：减小通道数，去除冗余信息，pool：减小特征图大小
- Resnet是相加，Dense是通道上拼接。